name: Weekly Argus Benchmark

on:
  schedule:
    # Every Sunday at 03:00 UTC
    - cron: "0 3 * * 0"
  workflow_dispatch:
    inputs:
      trials:
        description: "Trials per scenario"
        required: false
        default: "1"
      profile:
        description: "Gate profile (baseline|candidate|release|custom)"
        required: false
        default: "candidate"
      scenario_list:
        description: "Scenario list file path"
        required: false
        default: "scenarios/suites/sabotage_extended_v1.txt"
      matrix_scenario_list:
        description: "Scenario list for benchmark-matrix"
        required: false
        default: "scenarios/suites/complex_behavior_v1.txt"
      drift_strict:
        description: "Fail workflow when drift alert thresholds are exceeded (true|false)"
        required: false
        default: "false"
      drift_window:
        description: "Trend entries per model considered by drift check"
        required: false
        default: "12"
      drift_max_pass_drop:
        description: "Drift alert threshold for pass-rate drop"
        required: false
        default: "0.08"
      drift_max_severity_increase:
        description: "Drift alert threshold for avg total severity increase"
        required: false
        default: "0.75"
      drift_max_anomaly_increase:
        description: "Drift alert threshold for cross-trial anomaly count increase"
        required: false
        default: "3"

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 360
    permissions:
      contents: read

    env:
      OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
      MINIMAX_API_KEY: ${{ secrets.MINIMAX_API_KEY }}
      OPENROUTER_SITE_URL: ${{ vars.OPENROUTER_SITE_URL }}
      OPENROUTER_APP_NAME: ${{ vars.OPENROUTER_APP_NAME }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Verify required secrets
        run: |
          if [ -z "${OPENROUTER_API_KEY}" ]; then
            echo "Missing required secret: OPENROUTER_API_KEY"
            exit 1
          fi
          if [ -z "${MINIMAX_API_KEY}" ]; then
            echo "Missing required secret: MINIMAX_API_KEY"
            exit 1
          fi

      - name: Run benchmark pipeline
        run: |
          TRIALS="${{ github.event.inputs.trials || '1' }}"
          PROFILE="${{ github.event.inputs.profile || 'candidate' }}"
          SCENARIO_LIST="${{ github.event.inputs.scenario_list || 'scenarios/suites/sabotage_extended_v1.txt' }}"
          python -m argus.cli benchmark-pipeline \
            --scenario-list "${SCENARIO_LIST}" \
            --trials "${TRIALS}" \
            --seed 42 \
            --max-turns 6 \
            --profile "${PROFILE}" \
            --output-dir reports/suites \
            --trends-dir reports/suites/trends

      - name: Generate trend report
        run: |
          python -m argus.cli trend-report \
            --trend-dir reports/suites/trends \
            --window 12 \
            --output reports/suites/trends/weekly_trend_report.md

      - name: Generate drift summary
        run: |
          DRIFT_WINDOW="${{ github.event.inputs.drift_window || '12' }}"
          DRIFT_MAX_PASS_DROP="${{ github.event.inputs.drift_max_pass_drop || '0.08' }}"
          DRIFT_MAX_SEVERITY_INCREASE="${{ github.event.inputs.drift_max_severity_increase || '0.75' }}"
          DRIFT_MAX_ANOMALY_INCREASE="${{ github.event.inputs.drift_max_anomaly_increase || '3' }}"
          DRIFT_STRICT="${{ github.event.inputs.drift_strict || 'false' }}"

          # Scheduled runs are strict by default to catch regressions early.
          if [ "${{ github.event_name }}" = "schedule" ]; then
            DRIFT_STRICT="true"
          fi

          STRICT_FLAG=""
          if [ "${DRIFT_STRICT}" = "true" ]; then
            STRICT_FLAG="--strict"
          fi

          python -m argus.cli trend-drift-check \
            --trend-dir reports/suites/trends \
            --window "${DRIFT_WINDOW}" \
            --max-pass-drop "${DRIFT_MAX_PASS_DROP}" \
            --max-severity-increase "${DRIFT_MAX_SEVERITY_INCREASE}" \
            --max-anomaly-increase "${DRIFT_MAX_ANOMALY_INCREASE}" \
            --output reports/suites/trends/weekly_drift_report.md \
            --output-json reports/suites/trends/weekly_drift_report.json \
            ${STRICT_FLAG}

      - name: Run benchmark matrix
        run: |
          TRIALS="${{ github.event.inputs.trials || '1' }}"
          PROFILE="${{ github.event.inputs.profile || 'candidate' }}"
          MATRIX_SCENARIO_LIST="${{ github.event.inputs.matrix_scenario_list || 'scenarios/suites/complex_behavior_v1.txt' }}"
          python -m argus.cli benchmark-matrix \
            --scenario-list "${MATRIX_SCENARIO_LIST}" \
            --models MiniMax-M2.1 \
            --models stepfun/step-3.5-flash:free \
            --models openrouter/aurora-alpha \
            --trials "${TRIALS}" \
            --seed 42 \
            --max-turns 6 \
            --profile "${PROFILE}" \
            --output-dir reports/suites \
            --trends-dir reports/suites/trends

      - name: Generate behavior report
        run: |
          LATEST_MATRIX="$(ls -1t reports/suites/matrix/*_matrix.json | head -n1)"
          python -m argus.cli behavior-report \
            --matrix-json "${LATEST_MATRIX}" \
            --top-scenarios 6 \
            --excerpt-chars 240 \
            --output reports/suites/behavior/weekly_behavior_report.md

      - name: Generate suite and matrix visuals
        run: |
          find reports/suites -maxdepth 1 -name '*.json' -print0 | while IFS= read -r -d '' suite_json; do
            python -m argus.cli visualize-suite --suite-report "$suite_json" --output-dir reports/visuals
          done
          LATEST_MATRIX="$(ls -1t reports/suites/matrix/*_matrix.json | head -n1)"
          python -m argus.cli visualize-matrix \
            --matrix-json "${LATEST_MATRIX}" \
            --trend-dir reports/suites/trends \
            --window 12 \
            --output-dir reports/visuals

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: argus-benchmark-${{ github.run_id }}
          if-no-files-found: warn
          path: |
            reports/suites/*.json
            reports/suites/comparisons/*.md
            reports/suites/gates/*.json
            reports/suites/matrix/**/*.json
            reports/suites/matrix/**/*.md
            reports/suites/trends/*.jsonl
            reports/suites/trends/*.json
            reports/suites/trends/*.md
            reports/suites/behavior/*.md
            reports/visuals/**/*.svg
            reports/visuals/**/*.json
